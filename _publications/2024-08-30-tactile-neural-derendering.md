---
title: "Tactile Neural De-rendering"
collection: under_review
permalink: /publication/2024-08-30-tactile-neural-derendering
excerpt: 'We introduce Tactile Neural De-rendering, a novel approach that leverages a generative model to reconstruct a local 3D representation of an object based solely on its tactile signature.'
date: 2024-08-30
venue: 'Under Review'
image: "/images/projects/tactile_neural_derendering.png"
paperurl: 'https://arxiv.org/abs/2409.13923'
citation: 'J.A. Eyzaguirre, <b>Oller, M. </b>, & Fazeli, N. &quot; "Tactile Neural De-rendering". &quot; <i>2024</i>.'
---


![Tactile Neural De-rendering](/images/projects/tactile_neural_derendering.png)

Tactile sensing has proven to be an invaluable tool for enhancing robotic perception, particularly in scenarios where visual data is limited or unavailable. However, traditional methods for pose estimation using tactile data often rely on intricate modeling of sensor mechanics or estimation of contact patches, which can be cumbersome and inherently deterministic. In this work, we introduce Tactile Neural De-rendering, a novel approach that leverages a generative model to reconstruct a local 3D representation of an object based solely on its tactile signature. By rendering the object as though perceived by a virtual camera embedded at the fingertip, our method provides a more intuitive and flexible representation of the tactile data. This 3D reconstruction not only facilitates precise pose estimation but also allows for the quantification of uncertainty, providing a robust framework for tactile-based perception in robotics.

Project website: [https://www.mmintlab.com/tactile-neural-derendering/](https://www.mmintlab.com/research/tactile-neural-derendering/)


[Download paper here](https://arxiv.org/abs/2409.13923)
